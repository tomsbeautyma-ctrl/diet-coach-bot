// server.js â€” ESM ("type": "module")
// ä¾å­˜: express, @line/bot-sdk, openai

import express from "express";
import { Client, middleware as lineMW } from "@line/bot-sdk";
import OpenAI from "openai";

const PORT = process.env.PORT || 10000;

// ------------------------ åŸºæœ¬ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— ------------------------
const app = express();
// â€»LINEç½²åæ¤œè¨¼ã®ãŸã‚ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã® express.json() ã¯å…¥ã‚Œãªã„

// Health checks
app.get("/", (_req, res) => res.status(200).send("alive"));
app.get("/health", (_req, res) => res.status(200).send("ok"));

// LINE SDK
const lineConfig = {
  channelAccessToken: process.env.LINE_CHANNEL_ACCESS_TOKEN,
  channelSecret: process.env.LINE_CHANNEL_SECRET,
};
const lineClient = new Client(lineConfig);

// DeepInfra (OpenAIäº’æ›)
const ai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,         // DeepInfraã®APIã‚­ãƒ¼ (hf_...)
  baseURL: process.env.OPENAI_BASE_URL,       // ä¾‹: https://api.deepinfra.com/v1/openai
});

// ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šGETã§ç–é€šç¢ºèª
app.get("/webhook/line", (_req, res) => {
  res.status(200).send("LINE webhook endpoint is alive (POST required).");
});

// ------------------------ ç”»åƒå–å¾—ãƒ˜ãƒ«ãƒ‘ ------------------------
async function fetchLineImageBuffer(messageId) {
  const stream = await lineClient.getMessageContent(messageId); // Readable
  const chunks = [];
  for await (const chunk of stream) chunks.push(chunk);
  return Buffer.concat(chunks);
}

// ------------------------ Webhook (POST) ------------------------
app.post("/webhook/line", lineMW(lineConfig), async (req, res) => {
  try {
    const events = req.body?.events || [];

    await Promise.all(
      events.map(async (event) => {
        if (event.type !== "message") return;

        // ========== ç”»åƒè¨ºæ–­ ==========
        if (event.message.type === "image") {
          try {
            const buf = await fetchLineImageBuffer(event.message.id);
            const b64 = "data:image/jpeg;base64," + buf.toString("base64");

            const result = await ai.chat.completions.create({
              model: "meta-llama/Meta-Llama-3.2-90B-Vision-Instruct",
              messages: [
                {
                  role: "system",
                  content:
                    "ã‚ãªãŸã¯æ—¥æœ¬èªã§ç­”ãˆã‚‹éª¨æ ¼è¨ºæ–­ã®å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                    + "å†™çœŸã‹ã‚‰ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ/ã‚¦ã‚§ãƒ¼ãƒ–/ãƒŠãƒãƒ¥ãƒ©ãƒ«ã®å‚¾å‘(%)ã‚’æ¨å®šã—ã€"
                    + "ç‰¹å¾´ã€ä¼¼åˆã†ã‚·ãƒ«ã‚¨ãƒƒãƒˆã¨ç´ æã€é¿ã‘ãŸã„ä¾‹ã‚’ç°¡æ½”ã«3ã€œ6è¡Œã§ç­”ãˆã¦ãã ã•ã„ã€‚"
                },
                {
                  role: "user",
                  content: [
                    { type: "text", text: "ã“ã®äººã®éª¨æ ¼ã‚¿ã‚¤ãƒ—ã‚’è¨ºæ–­ã—ã¦ãã ã•ã„ã€‚" },
                    { type: "image_url", image_url: b64 },
                  ],
                },
              ],
              temperature: 0.2,
              max_tokens: 500,
            });

            const reply =
              result?.choices?.[0]?.message?.content?.trim() ||
              "ç”»åƒã‚’ã†ã¾ãè§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦æ˜ã‚‹ã„ã¨ã“ã‚ã§æ’®å½±ã—ã¦é€ã£ã¦ãã ã•ã„ğŸ“¸";

            await lineClient.replyMessage(event.replyToken, {
              type: "text",
              text: reply,
            });
          } catch (e) {
            console.error("Vision error:", e);
            await lineClient.replyMessage(event.replyToken, {
              type: "text",
              text: "ç”»åƒã®å–å¾—/è§£æã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ğŸ™",
            });
          }
          return; // ç”»åƒå‡¦ç†ã¯ã“ã“ã§çµ‚äº†
        }

        // ========== ãƒ†ã‚­ã‚¹ãƒˆå¿œç­” ==========
        if (event.message.type === "text") {
          const userText = (event.message.text || "").trim();

          const result = await ai.chat.completions.create({
            model: "meta-llama/Meta-Llama-3.1-8B-Instruct",
            messages: [
              {
                role: "system",
                content:
                  "You are a supportive Japanese fitness & styling assistant.",
              },
              { role: "user", content: userText },
            ],
            temperature: 0.4,
            max_tokens: 500,
          });

          const reply =
            result?.choices?.[0]?.message?.content?.trim() ||
            "ã†ã¾ãç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠé¡˜ã„ã—ã¾ã™ã€‚";

          await lineClient.replyMessage(event.replyToken, {
            type: "text",
            text: reply,
          });
        }
      })
    );

    // å¿…ãš 200 ã‚’è¿”ã™ï¼ˆLINEã®å†é€ã‚’é¿ã‘ã‚‹ï¼‰
    res.status(200).end();
  } catch (err) {
    console.error("Webhook error:", err);
    res.status(200).end();
  }
});

// ------------------------ å˜ä½“ãƒ†ã‚¹ãƒˆç”¨ ------------------------
app.get("/test/ai", async (_req, res) => {
  try {
    const r = await ai.chat.completions.create({
      model: "meta-llama/Meta-Llama-3.1-8B-Instruct",
      messages: [{ role: "user", content: "æ¥ç¶šãƒ†ã‚¹ãƒˆã€‚1è¡Œã§è¿”ç­”ã—ã¦ã€‚" }],
      max_tokens: 40,
    });
    res.json({ ok: true, text: r.choices?.[0]?.message?.content ?? "" });
  } catch (e) {
    console.error("âŒ /test/ai error:", e);
    res.status(500).json({ ok: false, name: e.name, message: e.message });
  }
});

