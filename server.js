// server.js â€” ESM ("type": "module")
// deps: express, @line/bot-sdk, openai

import express from "express";
import { Client, middleware as lineMW } from "@line/bot-sdk";
import OpenAI from "openai";

const PORT = Number(process.env.PORT || 10000);

// ====== ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©ï¼ˆåŸå› ã‚’å¿…ãšãƒ­ã‚°ï¼‰ ======
process.on("uncaughtException", (err) => {
  console.error("ğŸ”¥ uncaughtException:", err);
});
process.on("unhandledRejection", (reason, p) => {
  console.error("ğŸ”¥ unhandledRejection at:", p, "reason:", reason);
});

console.log("ğŸŸ¢ Booting server...");
console.log("ENV CHECK:", {
  PORT,
  HAS_LINE_ACCESS_TOKEN: Boolean(process.env.LINE_CHANNEL_ACCESS_TOKEN),
  HAS_LINE_SECRET: Boolean(process.env.LINE_CHANNEL_SECRET),
  HAS_OPENAI_KEY: Boolean(process.env.OPENAI_API_KEY),
  OPENAI_BASE_URL: process.env.OPENAI_BASE_URL,
});

// ====== App ======
const app = express(); // â€» express.json() ã¯ä»˜ã‘ãªã„ï¼ˆLINEç½²åã®ãŸã‚ï¼‰

// Health
app.get("/", (_req, res) => res.status(200).send("alive"));
app.get("/health", (_req, res) => res.status(200).send("ok"));

// ====== Safe init: LINE / OpenAI ã‚’ try-catch ã§ ======
let lineClient;
let lineConfig;
try {
  lineConfig = {
    channelAccessToken: process.env.LINE_CHANNEL_ACCESS_TOKEN || "",
    channelSecret: process.env.LINE_CHANNEL_SECRET || "",
  };
  lineClient = new Client(lineConfig);
  console.log("âœ… LINE SDK initialized");
} catch (e) {
  console.error("âŒ LINE SDK init failed:", e);
}

let ai;
try {
  ai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY || "",
    baseURL: process.env.OPENAI_BASE_URL, // ä¾‹: https://api.deepinfra.com/v1/openai
  });
  console.log("âœ… OpenAI(DeepInfra) client initialized");
} catch (e) {
  console.error("âŒ OpenAI client init failed:", e);
}

// Debug GET
app.get("/webhook/line", (_req, res) =>
  res.status(200).send("LINE webhook endpoint is alive (POST required).")
);

// ç”»åƒå–å¾—ãƒ˜ãƒ«ãƒ‘
async function fetchLineImageBuffer(messageId) {
  const stream = await lineClient.getMessageContent(messageId);
  const chunks = [];
  for await (const chunk of stream) chunks.push(chunk);
  return Buffer.concat(chunks);
}

// Webhook
app.post("/webhook/line", lineMW(lineConfig), async (req, res) => {
  try {
    const events = req.body?.events || [];
    console.log(`ğŸ“© Received ${events.length} events`);

    await Promise.all(
      events.map(async (event) => {
        if (event.type !== "message") return;

        // ç”»åƒ
        if (event.message.type === "image") {
          try {
            const buf = await fetchLineImageBuffer(event.message.id);
            const b64 = "data:image/jpeg;base64," + buf.toString("base64");

            const result = await ai.chat.completions.create({
              model: "meta-llama/Meta-Llama-3.2-90B-Vision-Instruct",
              messages: [
                {
                  role: "system",
                  content:
                    "ã‚ãªãŸã¯æ—¥æœ¬èªã§ç­”ãˆã‚‹éª¨æ ¼è¨ºæ–­ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                    + "å†™çœŸã‹ã‚‰ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ/ã‚¦ã‚§ãƒ¼ãƒ–/ãƒŠãƒãƒ¥ãƒ©ãƒ«ã®å‚¾å‘(%)ã‚’æ¨å®šã—ã€"
                    + "ç‰¹å¾´ãƒ»ä¼¼åˆã†ã‚·ãƒ«ã‚¨ãƒƒãƒˆ/ç´ æãƒ»é¿ã‘ãŸã„ä¾‹ã‚’3ã€œ6è¡Œã§ã€‚",
                },
                {
                  role: "user",
                  content: [
                    { type: "text", text: "ã“ã®äººã®éª¨æ ¼ã‚¿ã‚¤ãƒ—ã‚’è¨ºæ–­ã—ã¦ãã ã•ã„ã€‚" },
                    { type: "image_url", image_url: b64 },
                  ],
                },
              ],
              temperature: 0.2,
              max_tokens: 500,
            });

            const reply =
              result?.choices?.[0]?.message?.content?.trim() ||
              "ç”»åƒã‚’è§£æã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚";
            await lineClient.replyMessage(event.replyToken, {
              type: "text",
              text: reply,
            });
          } catch (e) {
            console.error("Vision error:", e);
            await lineClient.replyMessage(event.replyToken, {
              type: "text",
              text: "ç”»åƒã®å–å¾—/è§£æã§ã‚¨ãƒ©ãƒ¼ãŒèµ·ãã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠé¡˜ã„ã—ã¾ã™ğŸ™",
            });
          }
          return;
        }

        // ãƒ†ã‚­ã‚¹ãƒˆ
        if (event.message.type === "text") {
          const userText = (event.message.text || "").trim();
          const result = await ai.chat.completions.create({
            model: "meta-llama/Meta-Llama-3.1-8B-Instruct",
            messages: [
              {
                role: "system",
                content: "You are a supportive Japanese fitness & styling assistant.",
              },
              { role: "user", content: userText },
            ],
            temperature: 0.4,
            max_tokens: 500,
          });

          const reply =
            result?.choices?.[0]?.message?.content?.trim() ||
            "ã†ã¾ãç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠé¡˜ã„ã—ã¾ã™ã€‚";
          await lineClient.replyMessage(event.replyToken, {
            type: "text",
            text: reply,
          });
        }
      })
    );

    res.status(200).end();
  } catch (err) {
    console.error("Webhook error:", err);
    res.status(200).end(); // å†é€é˜²æ­¢
  }
});

// Listen
const server = app.listen(PORT, () => {
  console.log(`ğŸš€ Server started on port ${PORT}`);
});
server.on("error", (e) => console.error("âŒ server error:", e));
